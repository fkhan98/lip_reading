{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Prediction_using_lipnet_pytorch.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOUm9EvNm0+61SucTQAJSg8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a8ba299718574d9a89e1035cf2bf3795": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a74f8cc8779f44c28f18af259f6a4fe5",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_32d47cbbcecc4302b7e30a5120b8649b",
              "IPY_MODEL_9b19480d55aa4fafa11751c71fcaadf2",
              "IPY_MODEL_0bc3b9a14db04a68acc15ea12c605893"
            ]
          }
        },
        "a74f8cc8779f44c28f18af259f6a4fe5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "32d47cbbcecc4302b7e30a5120b8649b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8847711a8c08448a88eca5a2674d9293",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_06a8bb8af2ee46f68977e6f839b7b198"
          }
        },
        "9b19480d55aa4fafa11751c71fcaadf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0cf5e6eb4cb44ce08e34ed793c7fc3cf",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 89843225,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 89843225,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_31bfba5bf9474e728e065c5d2213e072"
          }
        },
        "0bc3b9a14db04a68acc15ea12c605893": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_065a634b8fe64caf9bf911ea2fa98f8b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 85.7M/85.7M [00:12&lt;00:00, 8.88MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e4c3524259004d60903a6a07b478b7ea"
          }
        },
        "8847711a8c08448a88eca5a2674d9293": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "06a8bb8af2ee46f68977e6f839b7b198": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0cf5e6eb4cb44ce08e34ed793c7fc3cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "31bfba5bf9474e728e065c5d2213e072": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "065a634b8fe64caf9bf911ea2fa98f8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e4c3524259004d60903a6a07b478b7ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fkhan98/lip_reading/blob/main/Prediction_using_lipnet_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "femWbkfZdjcj"
      },
      "source": [
        "!pip install face-alignment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6Jnz5m2SksF"
      },
      "source": [
        "## for lipnet model ##\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import numpy as np\n",
        "## for lipnet model ##\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import math\n",
        "import os\n",
        "import sys\n",
        "#from dataset import MyDataset\n",
        "import numpy as np\n",
        "import time\n",
        "import torch.optim as optim\n",
        "import re\n",
        "import json\n",
        "import tempfile\n",
        "import shutil\n",
        "import cv2\n",
        "import face_alignment\n",
        "from skimage import io\n",
        "from google.colab.patches import cv2_imshow\n",
        "from torch.utils.data import Dataset"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFWThyfkvdJ7"
      },
      "source": [
        "## Lipnet architecture "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eb6JXr_hTGjj"
      },
      "source": [
        "class LipNet(torch.nn.Module):\n",
        "    def __init__(self, dropout_p=0.5):\n",
        "        super(LipNet, self).__init__()\n",
        "        self.conv1 = nn.Conv3d(3, 32, (3, 5, 5), (1, 2, 2), (1, 2, 2))\n",
        "        self.pool1 = nn.MaxPool3d((1, 2, 2), (1, 2, 2))\n",
        "        \n",
        "        self.conv2 = nn.Conv3d(32, 64, (3, 5, 5), (1, 1, 1), (1, 2, 2))\n",
        "        self.pool2 = nn.MaxPool3d((1, 2, 2), (1, 2, 2))\n",
        "        \n",
        "        self.conv3 = nn.Conv3d(64, 96, (3, 3, 3), (1, 1, 1), (1, 1, 1))     \n",
        "        self.pool3 = nn.MaxPool3d((1, 2, 2), (1, 2, 2))\n",
        "        \n",
        "        self.gru1  = nn.GRU(96*4*8, 256, 1, bidirectional=True)\n",
        "        self.gru2  = nn.GRU(512, 256, 1, bidirectional=True)\n",
        "        \n",
        "        self.FC    = nn.Linear(512, 27+1)\n",
        "        self.dropout_p  = dropout_p\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)        \n",
        "        self.dropout3d = nn.Dropout3d(self.dropout_p)  \n",
        "        self._init()\n",
        "    \n",
        "    def _init(self):\n",
        "        \n",
        "        init.kaiming_normal_(self.conv1.weight, nonlinearity='relu')\n",
        "        init.constant_(self.conv1.bias, 0)\n",
        "        \n",
        "        init.kaiming_normal_(self.conv2.weight, nonlinearity='relu')\n",
        "        init.constant_(self.conv2.bias, 0)\n",
        "        \n",
        "        init.kaiming_normal_(self.conv3.weight, nonlinearity='relu')\n",
        "        init.constant_(self.conv3.bias, 0)        \n",
        "        \n",
        "        init.kaiming_normal_(self.FC.weight, nonlinearity='sigmoid')\n",
        "        init.constant_(self.FC.bias, 0)\n",
        "        \n",
        "        for m in (self.gru1, self.gru2):\n",
        "            stdv = math.sqrt(2 / (96 * 3 * 6 + 256))\n",
        "            for i in range(0, 256 * 3, 256):\n",
        "                init.uniform_(m.weight_ih_l0[i: i + 256],\n",
        "                            -math.sqrt(3) * stdv, math.sqrt(3) * stdv)\n",
        "                init.orthogonal_(m.weight_hh_l0[i: i + 256])\n",
        "                init.constant_(m.bias_ih_l0[i: i + 256], 0)\n",
        "                init.uniform_(m.weight_ih_l0_reverse[i: i + 256],\n",
        "                            -math.sqrt(3) * stdv, math.sqrt(3) * stdv)\n",
        "                init.orthogonal_(m.weight_hh_l0_reverse[i: i + 256])\n",
        "                init.constant_(m.bias_ih_l0_reverse[i: i + 256], 0)\n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout3d(x)\n",
        "        x = self.pool1(x)\n",
        "        \n",
        "        x = self.conv2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout3d(x)        \n",
        "        x = self.pool2(x)\n",
        "        \n",
        "        x = self.conv3(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout3d(x)        \n",
        "        x = self.pool3(x)\n",
        "        \n",
        "        # (B, C, T, H, W)->(T, B, C, H, W)\n",
        "        x = x.permute(2, 0, 1, 3, 4).contiguous()\n",
        "        # (B, C, T, H, W)->(T, B, C*H*W)\n",
        "        x = x.view(x.size(0), x.size(1), -1)\n",
        "        \n",
        "        self.gru1.flatten_parameters()\n",
        "        self.gru2.flatten_parameters()\n",
        "        \n",
        "        x, h = self.gru1(x)        \n",
        "        x = self.dropout(x)\n",
        "        x, h = self.gru2(x)   \n",
        "        x = self.dropout(x)\n",
        "                \n",
        "        x = self.FC(x)\n",
        "        x = x.permute(1, 0, 2).contiguous()\n",
        "        return x\n",
        "        "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfYqduFnXb5r"
      },
      "source": [
        "model = LipNet()\n",
        "model_dict = model.state_dict()\n",
        "model_dict\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGKyE8aiwagC"
      },
      "source": [
        "## dataset.MyDataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9YjB0sEyauz"
      },
      "source": [
        "import numpy as np\n",
        "import glob\n",
        "import time\n",
        "import cv2\n",
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "#from cvtransforms import *\n",
        "import torch\n",
        "import glob\n",
        "import re\n",
        "import copy\n",
        "import json\n",
        "import random\n",
        "import editdistance\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrOtHYzezC6Y"
      },
      "source": [
        "import random\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def HorizontalFlip(batch_img, p=0.5):\n",
        "    # (T, H, W, C)\n",
        "    if random.random() > p:\n",
        "        batch_img = batch_img[:,:,::-1,...]\n",
        "    return batch_img\n",
        "\n",
        "def ColorNormalize(batch_img):\n",
        "    batch_img = batch_img / 255.0\n",
        "    return batch_img"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdDeh3MRwXCT"
      },
      "source": [
        "class MyDataset(Dataset):\n",
        "    letters = [' ', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
        "\n",
        "    def __init__(self, video_path, anno_path, file_list, vid_pad, txt_pad, phase):\n",
        "        self.anno_path = anno_path\n",
        "        self.vid_pad = vid_pad\n",
        "        self.txt_pad = txt_pad\n",
        "        self.phase = phase\n",
        "        \n",
        "        with open(file_list, 'r') as f:\n",
        "            self.videos = [os.path.join(video_path, line.strip()) for line in f.readlines()]\n",
        "            \n",
        "        self.data = []\n",
        "        for vid in self.videos:\n",
        "            items = vid.split(os.path.sep)            \n",
        "            self.data.append((vid, items[-4], items[-1]))\n",
        "        \n",
        "                \n",
        "    def __getitem__(self, idx):\n",
        "        (vid, spk, name) = self.data[idx]\n",
        "        vid = self._load_vid(vid)\n",
        "        anno = self._load_anno(os.path.join(self.anno_path, spk, 'align', name + '.align'))\n",
        "\n",
        "        if(self.phase == 'train'):\n",
        "            vid = HorizontalFlip(vid)\n",
        "          \n",
        "        vid = ColorNormalize(vid)                   \n",
        "        \n",
        "        vid_len = vid.shape[0]\n",
        "        anno_len = anno.shape[0]\n",
        "        vid = self._padding(vid, self.vid_pad)\n",
        "        anno = self._padding(anno, self.txt_pad)\n",
        "        \n",
        "        return {'vid': torch.FloatTensor(vid.transpose(3, 0, 1, 2)), \n",
        "            'txt': torch.LongTensor(anno),\n",
        "            'txt_len': anno_len,\n",
        "            'vid_len': vid_len}\n",
        "            \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "        \n",
        "    def _load_vid(self, p): \n",
        "        files = os.listdir(p)\n",
        "        files = list(filter(lambda file: file.find('.jpg') != -1, files))\n",
        "        files = sorted(files, key=lambda file: int(os.path.splitext(file)[0]))\n",
        "        array = [cv2.imread(os.path.join(p, file)) for file in files]\n",
        "        array = list(filter(lambda im: not im is None, array))\n",
        "        array = [cv2.resize(im, (128, 64), interpolation=cv2.INTER_LANCZOS4) for im in array]\n",
        "        array = np.stack(array, axis=0).astype(np.float32)\n",
        "        return array\n",
        "    \n",
        "    def _load_anno(self, name):\n",
        "        with open(name, 'r') as f:\n",
        "            lines = [line.strip().split(' ') for line in f.readlines()]\n",
        "            txt = [line[2] for line in lines]\n",
        "            txt = list(filter(lambda s: not s.upper() in ['SIL', 'SP'], txt))\n",
        "        return MyDataset.txt2arr(' '.join(txt).upper(), 1)\n",
        "    \n",
        "    def _padding(self, array, length):\n",
        "        array = [array[_] for _ in range(array.shape[0])]\n",
        "        size = array[0].shape\n",
        "        for i in range(length - len(array)):\n",
        "            array.append(np.zeros(size))\n",
        "        return np.stack(array, axis=0)\n",
        "    \n",
        "    @staticmethod\n",
        "    def txt2arr(txt, start):\n",
        "        arr = []\n",
        "        for c in list(txt):\n",
        "            arr.append(MyDataset.letters.index(c) + start)\n",
        "        return np.array(arr)\n",
        "        \n",
        "    @staticmethod\n",
        "    def arr2txt(arr, start):\n",
        "        txt = []\n",
        "        for n in arr:\n",
        "            if(n >= start):\n",
        "                txt.append(MyDataset.letters[n - start])     \n",
        "        return ''.join(txt).strip()\n",
        "    \n",
        "    @staticmethod\n",
        "    def ctc_arr2txt(arr, start):\n",
        "        pre = -1\n",
        "        txt = []\n",
        "        for n in arr:\n",
        "            if(pre != n and n >= start):                \n",
        "                if(len(txt) > 0 and txt[-1] == ' ' and MyDataset.letters[n - start] == ' '):\n",
        "                    pass\n",
        "                else:\n",
        "                    txt.append(MyDataset.letters[n - start])                \n",
        "            pre = n\n",
        "        return ''.join(txt).strip()\n",
        "            \n",
        "    @staticmethod\n",
        "    def wer(predict, truth):        \n",
        "        word_pairs = [(p[0].split(' '), p[1].split(' ')) for p in zip(predict, truth)]\n",
        "        wer = [1.0*editdistance.eval(p[0], p[1])/len(p[1]) for p in word_pairs]\n",
        "        return wer\n",
        "        \n",
        "    @staticmethod\n",
        "    def cer(predict, truth):        \n",
        "        cer = [1.0*editdistance.eval(p[0], p[1])/len(p[1]) for p in zip(predict, truth)]\n",
        "        return cer"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yL7YTVwBvnz5"
      },
      "source": [
        "## Helper function for prediction "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAooIeOATei0"
      },
      "source": [
        "def get_position(size, padding=0.25):\n",
        "    \n",
        "    x = [0.000213256, 0.0752622, 0.18113, 0.29077, 0.393397, 0.586856, 0.689483, 0.799124,\n",
        "                    0.904991, 0.98004, 0.490127, 0.490127, 0.490127, 0.490127, 0.36688, 0.426036,\n",
        "                    0.490127, 0.554217, 0.613373, 0.121737, 0.187122, 0.265825, 0.334606, 0.260918,\n",
        "                    0.182743, 0.645647, 0.714428, 0.793132, 0.858516, 0.79751, 0.719335, 0.254149,\n",
        "                    0.340985, 0.428858, 0.490127, 0.551395, 0.639268, 0.726104, 0.642159, 0.556721,\n",
        "                    0.490127, 0.423532, 0.338094, 0.290379, 0.428096, 0.490127, 0.552157, 0.689874,\n",
        "                    0.553364, 0.490127, 0.42689]\n",
        "    \n",
        "    y = [0.106454, 0.038915, 0.0187482, 0.0344891, 0.0773906, 0.0773906, 0.0344891,\n",
        "                    0.0187482, 0.038915, 0.106454, 0.203352, 0.307009, 0.409805, 0.515625, 0.587326,\n",
        "                    0.609345, 0.628106, 0.609345, 0.587326, 0.216423, 0.178758, 0.179852, 0.231733,\n",
        "                    0.245099, 0.244077, 0.231733, 0.179852, 0.178758, 0.216423, 0.244077, 0.245099,\n",
        "                    0.780233, 0.745405, 0.727388, 0.742578, 0.727388, 0.745405, 0.780233, 0.864805,\n",
        "                    0.902192, 0.909281, 0.902192, 0.864805, 0.784792, 0.778746, 0.785343, 0.778746,\n",
        "                    0.784792, 0.824182, 0.831803, 0.824182]\n",
        "    \n",
        "    x, y = np.array(x), np.array(y)\n",
        "    \n",
        "    x = (x + padding) / (2 * padding + 1)\n",
        "    y = (y + padding) / (2 * padding + 1)\n",
        "    x = x * size\n",
        "    y = y * size\n",
        "    return np.array(list(zip(x, y)))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Q9GiMKYWGte"
      },
      "source": [
        "def cal_area(anno):\n",
        "    return (anno[:,0].max() - anno[:,0].min()) * (anno[:,1].max() - anno[:,1].min()) \n",
        "\n",
        "def output_video(p, txt, dst):\n",
        "    files = os.listdir(p)\n",
        "    files = sorted(files, key=lambda x: int(os.path.splitext(x)[0]))\n",
        "\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    \n",
        "    for file, line in zip(files, txt):\n",
        "        img = cv2.imread(os.path.join(p, file))\n",
        "        h, w, _ = img.shape\n",
        "        img = cv2.putText(img, line, (w//8, 11*h//12), font, 1.2, (0, 0, 0), 3, cv2.LINE_AA)\n",
        "        img = cv2.putText(img, line, (w//8, 11*h//12), font, 1.2, (255, 255, 255), 0, cv2.LINE_AA)  \n",
        "        h = h // 2\n",
        "        w = w // 2\n",
        "        img = cv2.resize(img, (w, h))     \n",
        "        cv2.imwrite(os.path.join(p, file), img)\n",
        "    \n",
        "    cmd = \"ffmpeg -y -i {}/%d.jpg -r 25 \\'{}\\'\".format(p, dst)\n",
        "    os.system(cmd)\n",
        "\n",
        "def transformation_from_points(points1, points2):\n",
        "    points1 = points1.astype(np.float64)\n",
        "    points2 = points2.astype(np.float64)\n",
        " \n",
        "    c1 = np.mean(points1, axis=0)\n",
        "    c2 = np.mean(points2, axis=0)\n",
        "    points1 -= c1\n",
        "    points2 -= c2\n",
        "    s1 = np.std(points1)\n",
        "    s2 = np.std(points2)\n",
        "    points1 /= s1\n",
        "    points2 /= s2\n",
        " \n",
        "    U, S, Vt = np.linalg.svd(points1.T * points2)\n",
        "    R = (U * Vt).T\n",
        "    return np.vstack([np.hstack(((s2 / s1) * R,\n",
        "                                       c2.T - (s2 / s1) * R * c1.T)),\n",
        "                         np.matrix([0., 0., 1.])])\n",
        "\n",
        "def load_video(file):\n",
        "    p = tempfile.mkdtemp()\n",
        "    cmd = 'ffmpeg -i \\'{}\\' -qscale:v 2 -r 25 \\'{}/%d.jpg\\''.format(file, p)\n",
        "    os.system(cmd)\n",
        "    \n",
        "    files = os.listdir(p)\n",
        "    files = sorted(files, key=lambda x: int(os.path.splitext(x)[0]))\n",
        "        \n",
        "    array = [cv2.imread(os.path.join(p, file)) for file in files]\n",
        "    \n",
        "    \n",
        "    array = list(filter(lambda im: not im is None, array))\n",
        "    #array = [cv2.resize(im, (100, 50), interpolation=cv2.INTER_LANCZOS4) for im in array]\n",
        "    \n",
        "    fa = face_alignment.FaceAlignment(face_alignment.LandmarksType._2D, flip_input=False, device='cuda')\n",
        "    points = [fa.get_landmarks(I) for I in array]\n",
        "    \n",
        "    front256 = get_position(256)\n",
        "    video = []\n",
        "    for point, scene in zip(points, array):\n",
        "        if(point is not None):\n",
        "            shape = np.array(point[0])\n",
        "            shape = shape[17:]\n",
        "            M = transformation_from_points(np.matrix(shape), np.matrix(front256))\n",
        "           \n",
        "            img = cv2.warpAffine(scene, M[:2], (256, 256))\n",
        "            (x, y) = front256[-20:].mean(0).astype(np.int32)\n",
        "            w = 160//2\n",
        "            img = img[y-w//2:y+w//2,x-w:x+w,...]\n",
        "            img = cv2.resize(img, (128, 64))\n",
        "            video.append(img)\n",
        "    \n",
        "    \n",
        "    video = np.stack(video, axis=0).astype(np.float32)\n",
        "    video = torch.FloatTensor(video.transpose(3, 0, 1, 2)) / 255.0\n",
        "\n",
        "    return video, p"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVxrHSHGWLn3"
      },
      "source": [
        "def ctc_decode(y):\n",
        "    y = y.argmax(-1)\n",
        "    t = y.size(0)\n",
        "    result = []\n",
        "    for i in range(t+1):\n",
        "        result.append(MyDataset.ctc_arr2txt(y[:i], start=1))\n",
        "    return result"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGug7Xe2W1Ox",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f4fb7bc-c57e-4c34-e263-833e3da83585"
      },
      "source": [
        "!rm -r /content/sample_data\n",
        "\n",
        "%cd /content/\n",
        "import os\n",
        "os.mkdir(\"sample_video\")\n",
        "os.mkdir(\"weights\")\n",
        "\n",
        "#face predicto dat file\n",
        "!gdown --id 1G-wQ6UjUrVgiBY_dSyZpXsEzdK_n_kTk\n",
        "\n",
        "\n",
        "%cd /content/sample_video/\n",
        "# video\n",
        "!gdown --id 13qL7pmXrgmTyPq5Om_q8pXhiX2ylAMIk\n",
        "\n",
        "# video's align \n",
        "!gdown --id 13pU8EVoHqkur-kYK54pdPZPpmeAD7k0t\n",
        "\n",
        "\n",
        "\n",
        "%cd /content/weights/\n",
        "## overlapped weights ##\n",
        "!gdown --id 1C3Xw3zfkMe88DYJPQwBx2sE2ijpfslsk\n",
        "## unseen weights ##\n",
        "!gdown --id 1ma0xWiCgHocbdFbmM_bD16vcYs1aGIk9\n",
        "\n",
        "\n",
        "%cd /content/"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1G-wQ6UjUrVgiBY_dSyZpXsEzdK_n_kTk\n",
            "To: /content/shape_predictor_68_face_landmarks.dat\n",
            "100% 99.7M/99.7M [00:02<00:00, 44.9MB/s]\n",
            "/content/sample_video\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=13qL7pmXrgmTyPq5Om_q8pXhiX2ylAMIk\n",
            "To: /content/sample_video/id2_vcd_swwp2s.mpg\n",
            "100% 397k/397k [00:00<00:00, 56.9MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=13pU8EVoHqkur-kYK54pdPZPpmeAD7k0t\n",
            "To: /content/sample_video/swwp2s.align\n",
            "100% 134/134 [00:00<00:00, 370kB/s]\n",
            "/content/weights\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1C3Xw3zfkMe88DYJPQwBx2sE2ijpfslsk\n",
            "To: /content/weights/LipNet_overlap_loss_0.07664558291435242_wer_0.04644484056248762_cer_0.019676921477851092.pt\n",
            "100% 26.6M/26.6M [00:00<00:00, 124MB/s] \n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ma0xWiCgHocbdFbmM_bD16vcYs1aGIk9\n",
            "To: /content/weights/LipNet_unseen_loss_0.44562849402427673_wer_0.1332580699113564_cer_0.06796452465503355.pt\n",
            "100% 26.6M/26.6M [00:00<00:00, 99.2MB/s]\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNJvt3wXWN51"
      },
      "source": [
        "weight_path = '/content/weights/LipNet_overlap_loss_0.07664558291435242_wer_0.04644484056248762_cer_0.019676921477851092.pt'\n",
        "sample_video_path = '/content/sample_video/id2_vcd_swwp2s.mpg'"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nde5bBxLaHJG"
      },
      "source": [
        "## Prediction ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdJLwebuZHGF",
        "outputId": "e7fc4659-4d44-407a-ba1a-e6bd86d9e793"
      },
      "source": [
        "model = LipNet()\n",
        "model = model.cuda()\n",
        "\n",
        "pretrained_dict = torch.load(weight_path,map_location=torch.device('cuda'))\n",
        "model_dict = model.state_dict()\n",
        "\n",
        "pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict.keys() and v.size() == model_dict[k].size()}\n",
        "missed_params = [k for k, v in model_dict.items() if not k in pretrained_dict.keys()]\n",
        "\n",
        "print('loaded params/tot params:{}/{}'.format(len(pretrained_dict),len(model_dict)))\n",
        "print('miss matched params:{}'.format(missed_params))\n",
        "\n",
        "model_dict.update(pretrained_dict)\n",
        "model.load_state_dict(model_dict)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded params/tot params:24/24\n",
            "miss matched params:[]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgfMrwNbZMxY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168,
          "referenced_widgets": [
            "a8ba299718574d9a89e1035cf2bf3795",
            "a74f8cc8779f44c28f18af259f6a4fe5",
            "32d47cbbcecc4302b7e30a5120b8649b",
            "9b19480d55aa4fafa11751c71fcaadf2",
            "0bc3b9a14db04a68acc15ea12c605893",
            "8847711a8c08448a88eca5a2674d9293",
            "06a8bb8af2ee46f68977e6f839b7b198",
            "0cf5e6eb4cb44ce08e34ed793c7fc3cf",
            "31bfba5bf9474e728e065c5d2213e072",
            "065a634b8fe64caf9bf911ea2fa98f8b",
            "e4c3524259004d60903a6a07b478b7ea"
          ]
        },
        "outputId": "35b5a2cf-c236-4130-ee6d-73fe14f67c6f"
      },
      "source": [
        "video, img_p = load_video(sample_video_path)\n",
        "y = model(video[None,...].cuda())\n",
        "txt = ctc_decode(y[0])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://www.adrianbulat.com/downloads/python-fan/s3fd-619a316812.pth\" to /root/.cache/torch/hub/checkpoints/s3fd-619a316812.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a8ba299718574d9a89e1035cf2bf3795",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/85.7M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://www.adrianbulat.com/downloads/python-fan/2DFAN4-cd938726ad.zip\" to /root/.cache/torch/hub/checkpoints/2DFAN4-cd938726ad.zip\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1244b61696724bb2b111cae90e8c3757",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/91.9M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXBbbKUd7OS1",
        "outputId": "5eba10d2-6e60-47dc-9f71-489c2422b5d3"
      },
      "source": [
        "y.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 75, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIKZ6e3-ZVS7"
      },
      "source": [
        "output_video(img_p, txt, \"/content/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xD4OF1m4v4ND",
        "outputId": "519e4436-d3f6-40db-90ff-880d1eb70222"
      },
      "source": [
        "txt[75]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'SET WHITE WITH P TWO SOON'"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tk8hmf474aHy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}